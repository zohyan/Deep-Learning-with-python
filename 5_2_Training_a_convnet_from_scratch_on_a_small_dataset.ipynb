{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Training a convnet from scratch on a small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having to train an image-classification model using very little data is a common situation, which you’ll likely encounter in practice if you ever do computer vision in a professional context. __A “few” samples can mean anywhere from a few hundred to a\n",
    "few tens of thousands of images__. As a practical example, we’ll focus on classifying images as dogs or cats, in a dataset containing 4,000 pictures of cats and dogs (2,000 cats, 2,000 dogs). We’ll use 2,000 pictures for training—1,000 for validation, and 1,000 for testing.\n",
    "\n",
    "In this section, we’ll review one basic strategy to tackle this problem: training a new model from scratch using what little data you have. You’ll start by naively training a small convnet on the 2,000 training samples, without any regularization, to set a baseline for what can be achieved. This will get you to a classification accuracy of 71%. At that point, the main issue will be overfitting. Then we’ll introduce __data augmentation__, a __powerful technique for mitigating overfitting in computer vision__. By using data augmentation, you’ll improve the network to reach an accuracy of 82%.\n",
    "\n",
    "In the next section, we’ll review __two more essential techniques__ for applying deep learning to small datasets: __feature extraction with a pretrained network__ (which will get you to an accuracy of 90% to 96%) and __fine-tuning a pretrained network (this will get you to a final accuracy of 97%)__. Together, these __three strategies__—training a small model from scratch, doing feature extraction using a pretrained model, and fine-tuning a pretrained model—will constitute your future __toolbox for tackling the problem of performing image classification with small datasets__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.1 The relevance of deep learning for small-data problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll sometimes hear that deep learning only works when lots of data is available. This is valid in part: one fundamental __characteristic of deep learning__ is that it can __find interesting features in the training data on its own__, without any need for manual feature engineering, and this can only be achieved when lots of training examples are available. This is __especially true for problems where the input samples are very highdimensional, like images__.\n",
    "\n",
    "But what __constitutes lots of samples is relative__—__relative to the size and depth of the network you’re trying to train__, for starters. It isn’t possible to train a convnet to solve a complex problem with just a few tens of samples, but a few hundred can potentially suffice if the model is small and well regularized and the task is simple. Because convnets learn local, translation-invariant features, they’re highly data efficient on perceptual problems. Training a convnet from scratch on a very small image dataset will still yield reasonable results despite a relative lack of data, without the need for any custom\n",
    "feature engineering. You’ll see this in action in this section.\n",
    "\n",
    "What’s more, deep-learning models are by nature __highly repurposable__: __you can take__, say, an __image-classification__ or __speech-to-text model trained on a large-scale dataset__ and __reuse it on a significantly different problem with only minor changes__. Specifically, in the case of computer vision, many pretrained models (usually trained on the Image-Net dataset) are now publicly available for download and can be used to bootstrap powerful vision models out of very little data. That’s what you’ll do in the next section. Let’s start by getting your hands on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2 Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dogs vs. Cats dataset that you’ll use isn’t packaged with Keras. It was made available by Kaggle as part of a computer-vision competition in late 2013, back when convnets weren’t mainstream. You can download the original dataset from www.kaggle\n",
    ".com/c/dogs-vs-cats/data (you’ll need to create a Kaggle account if you don’t already have one—don’t worry, the process is painless).\n",
    "\n",
    "Unsurprisingly, the __dogs-versus-cats Kaggle competition__ in 2013 was __won by entrants who used convnets__. The best entries achieved up to 95% accuracy. In this example, you’ll get fairly close to this accuracy (in the next section), even though you’ll train your models on less than 10% of the data that was available to the competitors.\n",
    "\n",
    "This dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543 MB (compressed). After downloading and uncompressing it, you’ll create a new dataset containing three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 500 samples of each class.\n",
    "\n",
    "Following is the code to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying images to training, validation, and test directories\n",
    "import os, shutil\n",
    "\n",
    "# Path to the directory where the original dataset was uncompressed\n",
    "original_dataset_dir = 'dogs_vs_cats'\n",
    "\n",
    "# Directory where you’ll store your smaller dataset\n",
    "base_dir = 'dogs_vs_cats_small'\n",
    "\n",
    "os.mkdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for the training,\n",
    "# validation, and test splits\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "os.mkdir(validation_dir)\n",
    "\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory with training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "os.mkdir(train_cats_dir)\n",
    "\n",
    "# Directory with training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "os.mkdir(train_dogs_dir)\n",
    "\n",
    "# Directory with validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "os.mkdir(validation_cats_dir)\n",
    "\n",
    "# Directory with validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "os.mkdir(validation_dogs_dir)\n",
    "\n",
    "# Directory with test cat pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "os.mkdir(test_cats_dir)\n",
    "\n",
    "# Directory with test dog pictures\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "os.mkdir(test_dogs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copies the first 1,000 cat images to train_cats_dir\n",
    "train_prefix = 'train\\\\train\\\\'\n",
    "# test_prefix = 'test1\\\\test1\\\\'\n",
    "\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, train_prefix, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copies the next 500 cat images to validation_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, train_prefix, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copies the next 500 cat images to test_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, train_prefix, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copies the first 1,000 dog images to train_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, train_prefix, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copies the next 500 dog images to validation_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, train_prefix, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copies the next 500 dog images to test_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, train_prefix, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let’s count how many pictures are in each training split (train/validation/\n",
    "test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training cat images: 1000\n"
     ]
    }
   ],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training dog images: 1000\n"
     ]
    }
   ],
   "source": [
    "print('total training dog images:', len(os.listdir(train_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation cat images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation dog images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test cat images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total test cat images:', len(os.listdir(test_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test dog images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total test dog images:', len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you do indeed have 2,000 training images, 1,000 validation images, and 1,000 test images. Each split contains the same number of samples from each class: this is a balanced __binary-classification problem__, which means classification accuracy will be an appropriate measure of success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.3 Building your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You built a small convnet for MNIST in the previous example, so you should be familiar with such convnets. You’ll reuse the same general structure: the convnet will be a stack of alternated Conv2D (with relu activation) and MaxPooling2D layers.\n",
    "\n",
    "But because you’re dealing with bigger images and a more complex problem, you’ll make your network larger, accordingly: it will have one more Conv2D + MaxPooling2D stage. This serves both __to augment the capacity of the network__ and __to further reduce the size of the feature maps__ so they aren’t overly large when you reach the Flatten layer. Here, because you start from inputs of size 150 × 150 (a somewhat arbitrary choice), you end up with feature maps of size 7 × 7 just before the Flatten layer.\n",
    "\n",
    "NOTE The depth of the feature maps progressively increases in the network (from 32 to 128), whereas the size of the feature maps decreases (from 148 × 148 to 7 × 7). This is a pattern you’ll see in almost all convnets.\n",
    "\n",
    "Because you’re attacking a binary-classification problem, you’ll end the network with a single unit (a Dense layer of size 1) and a sigmoid activation. This unit will encode the probability that the network is looking at one class or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yannickr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Instantiating a small convnet for dogs vs. cats classification\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at how the dimensions of the feature maps change with every successive layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the compilation step, you’ll go with the RMSprop optimizer, as usual. Because you ended the network with a single sigmoid unit, you’ll use binary crossentropy as the\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.4 Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know by now, data should be formatted into appropriately preprocessed floatingpoint tensors before being fed into the network. Currently, the data sits on a drive as JPEG files, so the steps for getting it into the network are roughly as follows:\n",
    "1. Read the picture files.\n",
    "2. Decode the JPEG content to RGB grids of pixels.\n",
    "3. Convert these into floating-point tensors.\n",
    "4. Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know, neural networks prefer to deal with small input values).\n",
    "\n",
    "It may seem a bit daunting, but fortunately __Keras has utilities to take care of these steps automatically__. Keras has a module with image-processing helper tools, located at __keras.preprocessing.image__. In particular, it contains the class __ImageDataGenerator__, which lets you quickly set up Python generators that can automatically __turn image files on disk into batches of preprocessed tensors__. This is what you’ll use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Using ImageDataGenerator to read images from directories\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Rescales all images by 1/255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150), # Resizes all images to 150 × 150\n",
    "    batch_size=20,\n",
    "    class_mode='binary' # Because you use binary_crossentropy loss, you need binary labels.\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150), # Resizes all images to 150 × 150\n",
    "    batch_size=20,\n",
    "    class_mode='binary' # Because you use binary_crossentropy loss, you need binary labels.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the output of one of these generators: it yields batches of 150 × 150 RGB images (shape (20, 150, 150, 3)) and binary labels (shape (20,)). There are 20 samples in each batch (the batch size). Note that the generator yields these batches indefinitely: it loops endlessly over the images in the target folder. For this reason, you need to break the iteration loop at some point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model :\n",
    "1. Method: __fit_generator__ .\n",
    "2. Argument : __train_data__ (as a __generator__).\n",
    "3. Argument : __steps_per_epoch__ (Number of iteration per epochin train data).\n",
    "4. Argument : __validation_data__ (as a __generator__).\n",
    "5. Argument : __validation_steps__ (Number of iteration per epoch in validation data).\n",
    "\n",
    "(1) Let’s fit the model to the data using the generator. You do so __using the fit_generator__ method, the equivalent of fit for __data generators__ like this one. \n",
    "\n",
    "(2) It expects as its first argument a Python generator that will yield batches of inputs and targets indefinitely, like this one does.\n",
    "\n",
    "(3) Because the data is being generated endlessly, the Keras model needs to know how many samples to draw from the generator before declaring an epoch over (Number of iteration per epoch = __steps_per_epoch__).This is the role of the __steps_per_epoch__ argument: after having drawn __steps_per_epoch__ batches from the generator—that is, after having run for __steps_per_epoch__ gradient descent steps—the fitting process will go to the next epoch. In this case, batches are 20 samples, so it will take 100 batches until you see your target of 2,000 samples.(2,000 samples = 20 samples * 100 batches => __steps_per_epoch__ = __100__).\n",
    "\n",
    "(4) When using fit_generator, you can pass a __validation_data__ argument, much as with the fit method. It’s important to note that this argument is allowed to be a data generator, but it could also be a tuple of Numpy arrays. \n",
    "\n",
    "(5) If you pass a generator as validation_data, then this generator is expected to yield batches of validation data endlessly; thus you should also specify the __validation_steps__ argument, which tells the process how many batches to draw from the validation generator for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yannickr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 114s 1s/step - loss: 0.6921 - acc: 0.5320 - val_loss: 0.6665 - val_acc: 0.6440\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 112s 1s/step - loss: 0.6594 - acc: 0.5980 - val_loss: 0.6596 - val_acc: 0.5540\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 110s 1s/step - loss: 0.6092 - acc: 0.6680 - val_loss: 0.5594 - val_acc: 0.7230\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 110s 1s/step - loss: 0.5599 - acc: 0.7015 - val_loss: 0.5470 - val_acc: 0.6900\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 111s 1s/step - loss: 0.5376 - acc: 0.7240 - val_loss: 0.5263 - val_acc: 0.7080\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 112s 1s/step - loss: 0.5086 - acc: 0.7425 - val_loss: 0.6112 - val_acc: 0.6400\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 113s 1s/step - loss: 0.4807 - acc: 0.7735 - val_loss: 0.4382 - val_acc: 0.8130\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 109s 1s/step - loss: 0.4552 - acc: 0.7840 - val_loss: 0.4126 - val_acc: 0.8130\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 109s 1s/step - loss: 0.4289 - acc: 0.8065 - val_loss: 0.3775 - val_acc: 0.8370\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.4018 - acc: 0.8160 - val_loss: 0.3674 - val_acc: 0.8450\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 110s 1s/step - loss: 0.3824 - acc: 0.8380 - val_loss: 0.3605 - val_acc: 0.8480\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 110s 1s/step - loss: 0.3547 - acc: 0.8485 - val_loss: 0.2899 - val_acc: 0.8850\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 108s 1s/step - loss: 0.3293 - acc: 0.8550 - val_loss: 0.3605 - val_acc: 0.8130\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 110s 1s/step - loss: 0.3035 - acc: 0.8735 - val_loss: 0.4574 - val_acc: 0.7580\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 116s 1s/step - loss: 0.2809 - acc: 0.8870 - val_loss: 0.2330 - val_acc: 0.9060\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 115s 1s/step - loss: 0.2570 - acc: 0.8960 - val_loss: 0.2124 - val_acc: 0.9250\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 111s 1s/step - loss: 0.2363 - acc: 0.9135 - val_loss: 0.2244 - val_acc: 0.9070\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 112s 1s/step - loss: 0.2101 - acc: 0.9230 - val_loss: 0.1462 - val_acc: 0.9600\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 109s 1s/step - loss: 0.1944 - acc: 0.9250 - val_loss: 0.1469 - val_acc: 0.9610\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 110s 1s/step - loss: 0.1636 - acc: 0.9405 - val_loss: 0.1193 - val_acc: 0.9700\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 109s 1s/step - loss: 0.1595 - acc: 0.9420 - val_loss: 0.1051 - val_acc: 0.9710\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 110s 1s/step - loss: 0.1385 - acc: 0.9495 - val_loss: 0.0856 - val_acc: 0.9800\n",
      "Epoch 23/30\n",
      " 47/100 [=============>................] - ETA: 48s - loss: 0.1136 - acc: 0.9649"
     ]
    }
   ],
   "source": [
    "# Fitting the model using a batch generator\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s good practice to always save your models after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model.save('cats_and_dogs_small_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s plot the loss and accuracy of the model over the training and validation data during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying curves of loss and accuracy during training\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots are characteristic of overfitting. The training accuracy increases linearly over time, until it reaches nearly 100%, whereas the validation accuracy stalls at 70–72%. The validation loss reaches its minimum after only five epochs and then stalls, whereas the training loss keeps decreasing linearly until it reaches nearly 0.\n",
    "\n",
    "Because you have relatively few training samples (2,000), overfitting will be your number-one concern. You already know about a number of techniques that can help mitigate overfitting, such as dropout and weight decay (L2 regularization). We’re now\n",
    "going to work with a new one, specific to computer vision and used almost universally when processing images with deep-learning models: __data augmentation__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.5 Using data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
